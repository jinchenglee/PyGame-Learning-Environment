
------------------------------
log_3: 09/22/2016 
------------------------------
Overnight 8 hrs training. Amazing result!

Changes:
1. As log_3 already shows good sign of learning. Starting epsilon is set to 0.5.
2. Restored the scene where log_3 ends as training starting point. Log file shows initial regression, but eventually it skyrocketed.
3. Fixed epsilon not descreasing at right step bug in last round. It turns out the biggest side effect is training turns out to be 2x slower (well understood as last round most occurrence were skipping training step due to large epsilon).
4. 50,000 episodes took ~8 hrs. 
5. Tensorboard shows cost shrink to <50, comparing to pretty large (<5000?) in last round.
6. Might want to increase STEP to larger number as currently capped at 100 provides no training data for longer snake body case that started occurring after this round. 

18:23:28.078647 episode:  0 / 50000  Evaluation Average Reward  45.6 Epsilon  0.5
18:28:39.067795 episode:  1000 / 50000  Evaluation Average Reward  44.0 Epsilon  0.4902
18:34:12.632748 episode:  2000 / 50000  Evaluation Average Reward  22.4 Epsilon  0.48040000000000005
18:39:38.139624 episode:  3000 / 50000  Evaluation Average Reward  25.2 Epsilon  0.4706000000000001
18:44:45.587622 episode:  4000 / 50000  Evaluation Average Reward  18.5 Epsilon  0.4608000000000001
18:50:26.580619 episode:  5000 / 50000  Evaluation Average Reward  -2.1 Epsilon  0.4510000000000001
18:56:29.614619 episode:  6000 / 50000  Evaluation Average Reward  -12.5 Epsilon  0.44120000000000015
19:02:22.015741 episode:  7000 / 50000  Evaluation Average Reward  40.4 Epsilon  0.43140000000000017
19:08:30.735617 episode:  8000 / 50000  Evaluation Average Reward  20.2 Epsilon  0.4216000000000002
...
00:42:21.790737 episode:  44000 / 50000  Evaluation Average Reward  314.3 Epsilon  0.06880000000000056
00:58:45.220775 episode:  45000 / 50000  Evaluation Average Reward  365.4 Epsilon  0.05900000000000055
01:12:30.911623 episode:  46000 / 50000  Evaluation Average Reward  345.8 Epsilon  0.04920000000000055
01:28:50.394628 episode:  47000 / 50000  Evaluation Average Reward  317.0 Epsilon  0.039400000000000546
01:47:09.349920 episode:  48000 / 50000  Evaluation Average Reward  391.7 Epsilon  0.029600000000000546
02:06:22.258638 episode:  49000 / 50000  Evaluation Average Reward  366.0 Epsilon  0.019800000000000546
------------------------------
log_3: 09/21/2016 
------------------------------
1. Change snake game reward on food-eating from 5.0 to 50. 
2. Add scalar_summary and graph variables into DQN net that allows monitoring cost and net topology/hierarchy in tensorboard.

Training for 100,000 episodes (6 hrs) while testing every 1,000 episodes. 

One interesting thig is that there's an unintended bug about Epsilon decreasing at 1/1000 pace after every testing. Was intended to decrease to 0.1, but only decreased to 0.8. Not sure how this affects the training process, good or bad. Seems positive. 

Good progress witnessed in log as below. 

07:32:17.325998 episode:  0 / 100000  Evaluation Average Reward  -8.2 Epsilon  0.9
07:35:14.414888 episode:  1000 / 100000  Evaluation Average Reward  -65.0 Epsilon  0.8991100000000001
07:38:22.090960 episode:  2000 / 100000  Evaluation Average Reward  -95.0 Epsilon  0.8982200000000001
07:41:28.801890 episode:  3000 / 100000  Evaluation Average Reward  -35.0 Epsilon  0.8973300000000002
07:44:34.591976 episode:  4000 / 100000  Evaluation Average Reward  -73.6 Epsilon  0.8964400000000002
...
12:56:30.844020 episode:  95000 / 100000  Evaluation Average Reward  11.2 Epsilon  0.8154500000000051
13:00:12.513006 episode:  96000 / 100000  Evaluation Average Reward  81.0 Epsilon  0.8145600000000052
13:03:43.000891 episode:  97000 / 100000  Evaluation Average Reward  80.5 Epsilon  0.8136700000000052
13:07:15.963893 episode:  98000 / 100000  Evaluation Average Reward  30.3 Epsilon  0.8127800000000053
13:10:52.995888 episode:  99000 / 100000  Evaluation Average Reward  59.2 Epsilon  0.8118900000000053



------------------------------
log_2: 09/17/2016 
------------------------------
1. Fixed epsilon not decreasing bug.
2. Change every move reward to -1.0 if not food hit, was -0.01 which might have caused circling running witnessed in review as too little negative reward by doing that.
3. Add code to store replay_buffer contents as well. Use pickle and gzip version.

Not much progress after all these changes. 20,000 episodes ran (details in log_2), no big difference was witnessed in "Evaluation Average Reward".



------------------------------
log_1: 
------------------------------

Training log file that ran with later found bug: not decreasing epsilon as training goes by. Thus valuable trained Q-values not being exploited in time.
Very early exploration is effective, but soon exploration led the training into weeds. 

good trend =>    22:15:41.225640 episode:  0 / 10000  Evaluation Average Reward  -4.611 Epsilon  0.9
good trend =>    22:15:58.682509 episode:  100 / 10000  Evaluation Average Reward  -4.718 Epsilon  0.8911
good trend =>    22:16:21.285550 episode:  200 / 10000  Evaluation Average Reward  -0.4279999999999914 Epsilon  0.8822
good trend =>    22:16:50.319592 episode:  300 / 10000  Evaluation Average Reward  0.2960000000000121 Epsilon  0.8733
go in weeds=>    22:17:18.556561 episode:  400 / 10000  Evaluation Average Reward  -0.250999999999988 Epsilon  0.8644
go in weeds=>    22:17:50.887580 episode:  500 / 10000  Evaluation Average Reward  -0.9999999999999831 Epsilon  0.8554999999999999
go in weeds=>    22:18:22.305511 episode:  600 / 10000  Evaluation Average Reward  2.6339999999999923 Epsilon  0.8465999999999999
go in weeds=>    22:18:54.699514 episode:  700 / 10000  Evaluation Average Reward  1.5000000000000209 Epsilon  0.8376999999999999
go in weeds=>    22:19:29.123635 episode:  800 / 10000  Evaluation Average Reward  1.0190000000000188 Epsilon  0.8287999999999999
go in weeds=>    22:20:04.238512 episode:  900 / 10000  Evaluation Average Reward  1.199040866595169e-14 Epsilon  0.8198999999999999
go in weeds=>    22:20:36.667577 episode:  1000 / 10000  Evaluation Average Reward  -0.910999999999994 Epsilon  0.8109999999999998
