
------------------------------
log_6: 09/24/2016 
------------------------------
18 hrs training, marginal improvements witnessed in review replay. 
Changes:
1. Increased episode limit to 100,000, increased STEP limit to 500.
2. Start epsilon at 0.5 and decrease evenly along 100 steps (every 1000 episodes).

Round 5 replay:
Game over after  117  steps. Snake length =  17
Game over after  115  steps. Snake length =  16
Game over after  128  steps. Snake length =  19
07:55:29.460919 episode:  0 Evaluation Average Reward:  645.6666666666666
Game over after  82  steps. Snake length =  13
Game over after  91  steps. Snake length =  13
Game over after  80  steps. Snake length =  10
07:55:46.361912 episode:  1 Evaluation Average Reward:  414.6666666666667
Game over after  54  steps. Snake length =  7
Game over after  49  steps. Snake length =  10
Game over after  50  steps. Snake length =  9
07:55:56.663051 episode:  2 Evaluation Average Reward:  281.3333333333333

Round 6 replay:
Game over after  181  steps. Snake length =  21
Game over after  171  steps. Snake length =  23
Game over after  157  steps. Snake length =  21
07:56:49.817095 episode:  0 Evaluation Average Reward:  812.6666666666666
Game over after  40  steps. Snake length =  7
Game over after  129  steps. Snake length =  18
Game over after  183  steps. Snake length =  21
07:57:13.254170 episode:  1 Evaluation Average Reward:  548.3333333333334
Game over after  120  steps. Snake length =  17
Game over after  89  steps. Snake length =  15
Game over after  111  steps. Snake length =  19
07:57:34.576166 episode:  2 Evaluation Average Reward:  642.3333333333334

------------------------------
log_5: 09/23/2016 
------------------------------
Overnight 9 hrs training. Decent progress witnessed: Average rewards moving from ~300 to ~500.
Main changes:
1. Increase STEP limit from 100 to 300 during training. This is because snake body gets longer and longer, STEP might limit the training process to get meaningful new data, i.e., body long enough but game exit due to step limit hit.
2. Epsilon starts from 0.5 before decreasing. 
3. Make game loss reward from -10 to -50 to punish more on the case snake hit into walls or its body, which was seen in reviews from previous training result. It seems doing its work as before this specific change, average rewards didn't change much after ~8 hrs training.
4. Strangely the cost metric in tensorboard doesn't seem to descrease, it first decreased from ~60 to ~35 then went back to ~50. Will keep an eye on it. 

22:49:09.689711 episode:  0 / 50000  Evaluation Average Reward  330.3333333333333 Epsilon  0.5
22:54:45.592753 episode:  1000 / 50000  Evaluation Average Reward  266.3333333333333 Epsilon  0.4902
22:59:50.440800 episode:  2000 / 50000  Evaluation Average Reward  303.0 Epsilon  0.48040000000000005
23:05:06.064819 episode:  3000 / 50000  Evaluation Average Reward  281.6666666666667 Epsilon  0.4706000000000001
23:10:25.395717 episode:  4000 / 50000  Evaluation Average Reward  212.33333333333334 Epsilon  0.4608000000000001
...
5:58:18.396785 episode:  45000 / 50000  Evaluation Average Reward  582.3333333333334 Epsilon  0.05900000000000055
06:16:55.475853 episode:  46000 / 50000  Evaluation Average Reward  568.6666666666666 Epsilon  0.04920000000000055
06:38:04.380719 episode:  47000 / 50000  Evaluation Average Reward  403.3333333333333 Epsilon  0.039400000000000546
07:00:00.636041 episode:  48000 / 50000  Evaluation Average Reward  529.0 Epsilon  0.029600000000000546
07:23:39.161947 episode:  49000 / 50000  Evaluation Average Reward  538.3333333333334 Epsilon  0.019800000000000546



------------------------------
log_4: 09/22/2016 
------------------------------
Overnight 8 hrs training. Amazing result!

Changes:
1. As log_3 already shows good sign of learning. Starting epsilon is set to 0.5.
2. Restored the scene where log_3 ends as training starting point. Log file shows initial regression, but eventually it skyrocketed.
3. Fixed epsilon not descreasing at right step bug in last round. It turns out the biggest side effect is training turns out to be 2x slower (well understood as last round most occurrence were skipping training step due to large epsilon).
4. 50,000 episodes took ~8 hrs. 
5. Tensorboard shows cost shrink to <50, comparing to pretty large (<5000?) in last round.
6. Might want to increase STEP to larger number as currently capped at 100 provides no training data for longer snake body case that started occurring after this round. 

18:23:28.078647 episode:  0 / 50000  Evaluation Average Reward  45.6 Epsilon  0.5
18:28:39.067795 episode:  1000 / 50000  Evaluation Average Reward  44.0 Epsilon  0.4902
18:34:12.632748 episode:  2000 / 50000  Evaluation Average Reward  22.4 Epsilon  0.48040000000000005
18:39:38.139624 episode:  3000 / 50000  Evaluation Average Reward  25.2 Epsilon  0.4706000000000001
18:44:45.587622 episode:  4000 / 50000  Evaluation Average Reward  18.5 Epsilon  0.4608000000000001
18:50:26.580619 episode:  5000 / 50000  Evaluation Average Reward  -2.1 Epsilon  0.4510000000000001
18:56:29.614619 episode:  6000 / 50000  Evaluation Average Reward  -12.5 Epsilon  0.44120000000000015
19:02:22.015741 episode:  7000 / 50000  Evaluation Average Reward  40.4 Epsilon  0.43140000000000017
19:08:30.735617 episode:  8000 / 50000  Evaluation Average Reward  20.2 Epsilon  0.4216000000000002
...
00:42:21.790737 episode:  44000 / 50000  Evaluation Average Reward  314.3 Epsilon  0.06880000000000056
00:58:45.220775 episode:  45000 / 50000  Evaluation Average Reward  365.4 Epsilon  0.05900000000000055
01:12:30.911623 episode:  46000 / 50000  Evaluation Average Reward  345.8 Epsilon  0.04920000000000055
01:28:50.394628 episode:  47000 / 50000  Evaluation Average Reward  317.0 Epsilon  0.039400000000000546
01:47:09.349920 episode:  48000 / 50000  Evaluation Average Reward  391.7 Epsilon  0.029600000000000546
02:06:22.258638 episode:  49000 / 50000  Evaluation Average Reward  366.0 Epsilon  0.019800000000000546
------------------------------
log_3: 09/21/2016 
------------------------------
1. Change snake game reward on food-eating from 5.0 to 50. 
2. Add scalar_summary and graph variables into DQN net that allows monitoring cost and net topology/hierarchy in tensorboard.

Training for 100,000 episodes (6 hrs) while testing every 1,000 episodes. 

One interesting thig is that there's an unintended bug about Epsilon decreasing at 1/1000 pace after every testing. Was intended to decrease to 0.1, but only decreased to 0.8. Not sure how this affects the training process, good or bad. Seems positive. 

Good progress witnessed in log as below. 

07:32:17.325998 episode:  0 / 100000  Evaluation Average Reward  -8.2 Epsilon  0.9
07:35:14.414888 episode:  1000 / 100000  Evaluation Average Reward  -65.0 Epsilon  0.8991100000000001
07:38:22.090960 episode:  2000 / 100000  Evaluation Average Reward  -95.0 Epsilon  0.8982200000000001
07:41:28.801890 episode:  3000 / 100000  Evaluation Average Reward  -35.0 Epsilon  0.8973300000000002
07:44:34.591976 episode:  4000 / 100000  Evaluation Average Reward  -73.6 Epsilon  0.8964400000000002
...
12:56:30.844020 episode:  95000 / 100000  Evaluation Average Reward  11.2 Epsilon  0.8154500000000051
13:00:12.513006 episode:  96000 / 100000  Evaluation Average Reward  81.0 Epsilon  0.8145600000000052
13:03:43.000891 episode:  97000 / 100000  Evaluation Average Reward  80.5 Epsilon  0.8136700000000052
13:07:15.963893 episode:  98000 / 100000  Evaluation Average Reward  30.3 Epsilon  0.8127800000000053
13:10:52.995888 episode:  99000 / 100000  Evaluation Average Reward  59.2 Epsilon  0.8118900000000053



------------------------------
log_2: 09/17/2016 
------------------------------
1. Fixed epsilon not decreasing bug.
2. Change every move reward to -1.0 if not food hit, was -0.01 which might have caused circling running witnessed in review as too little negative reward by doing that.
3. Add code to store replay_buffer contents as well. Use pickle and gzip version.

Not much progress after all these changes. 20,000 episodes ran (details in log_2), no big difference was witnessed in "Evaluation Average Reward".



------------------------------
log_1: 
------------------------------

Training log file that ran with later found bug: not decreasing epsilon as training goes by. Thus valuable trained Q-values not being exploited in time.
Very early exploration is effective, but soon exploration led the training into weeds. 

good trend =>    22:15:41.225640 episode:  0 / 10000  Evaluation Average Reward  -4.611 Epsilon  0.9
good trend =>    22:15:58.682509 episode:  100 / 10000  Evaluation Average Reward  -4.718 Epsilon  0.8911
good trend =>    22:16:21.285550 episode:  200 / 10000  Evaluation Average Reward  -0.4279999999999914 Epsilon  0.8822
good trend =>    22:16:50.319592 episode:  300 / 10000  Evaluation Average Reward  0.2960000000000121 Epsilon  0.8733
go in weeds=>    22:17:18.556561 episode:  400 / 10000  Evaluation Average Reward  -0.250999999999988 Epsilon  0.8644
go in weeds=>    22:17:50.887580 episode:  500 / 10000  Evaluation Average Reward  -0.9999999999999831 Epsilon  0.8554999999999999
go in weeds=>    22:18:22.305511 episode:  600 / 10000  Evaluation Average Reward  2.6339999999999923 Epsilon  0.8465999999999999
go in weeds=>    22:18:54.699514 episode:  700 / 10000  Evaluation Average Reward  1.5000000000000209 Epsilon  0.8376999999999999
go in weeds=>    22:19:29.123635 episode:  800 / 10000  Evaluation Average Reward  1.0190000000000188 Epsilon  0.8287999999999999
go in weeds=>    22:20:04.238512 episode:  900 / 10000  Evaluation Average Reward  1.199040866595169e-14 Epsilon  0.8198999999999999
go in weeds=>    22:20:36.667577 episode:  1000 / 10000  Evaluation Average Reward  -0.910999999999994 Epsilon  0.8109999999999998
