------------------------------
log_3: 09/21/2016 
------------------------------
1. Change snake game reward on food-eating from 5.0 to 50. 
2. Add scalar_summary and graph variables into DQN net that allows monitoring cost and net topology/hierarchy in tensorboard.

Training for 100,000 episodes (6 hrs) while testing every 1,000 episodes. 

One interesting thig is that there's an unintended bug about Epsilon decreasing at 1/1000 pace after every testing. Was intended to decrease to 0.1, but only decreased to 0.8. Not sure how this affects the training process, good or bad. Seems positive. 

Good progress witnessed in log as below. 

07:32:17.325998 episode:  0 / 100000  Evaluation Average Reward  -8.2 Epsilon  0.9
07:35:14.414888 episode:  1000 / 100000  Evaluation Average Reward  -65.0 Epsilon  0.8991100000000001
07:38:22.090960 episode:  2000 / 100000  Evaluation Average Reward  -95.0 Epsilon  0.8982200000000001
07:41:28.801890 episode:  3000 / 100000  Evaluation Average Reward  -35.0 Epsilon  0.8973300000000002
07:44:34.591976 episode:  4000 / 100000  Evaluation Average Reward  -73.6 Epsilon  0.8964400000000002
...
12:56:30.844020 episode:  95000 / 100000  Evaluation Average Reward  11.2 Epsilon  0.8154500000000051
13:00:12.513006 episode:  96000 / 100000  Evaluation Average Reward  81.0 Epsilon  0.8145600000000052
13:03:43.000891 episode:  97000 / 100000  Evaluation Average Reward  80.5 Epsilon  0.8136700000000052
13:07:15.963893 episode:  98000 / 100000  Evaluation Average Reward  30.3 Epsilon  0.8127800000000053
13:10:52.995888 episode:  99000 / 100000  Evaluation Average Reward  59.2 Epsilon  0.8118900000000053



------------------------------
log_2: 09/17/2016 
------------------------------
1. Fixed epsilon not decreasing bug.
2. Change every move reward to -1.0 if not food hit, was -0.01 which might have caused circling running witnessed in review as too little negative reward by doing that.
3. Add code to store replay_buffer contents as well. Use pickle and gzip version.

Not much progress after all these changes. 20,000 episodes ran (details in log_2), no big difference was witnessed in "Evaluation Average Reward".



------------------------------
log_1: 
------------------------------

Training log file that ran with later found bug: not decreasing epsilon as training goes by. Thus valuable trained Q-values not being exploited in time.
Very early exploration is effective, but soon exploration led the training into weeds. 

good trend =>    22:15:41.225640 episode:  0 / 10000  Evaluation Average Reward  -4.611 Epsilon  0.9
good trend =>    22:15:58.682509 episode:  100 / 10000  Evaluation Average Reward  -4.718 Epsilon  0.8911
good trend =>    22:16:21.285550 episode:  200 / 10000  Evaluation Average Reward  -0.4279999999999914 Epsilon  0.8822
good trend =>    22:16:50.319592 episode:  300 / 10000  Evaluation Average Reward  0.2960000000000121 Epsilon  0.8733
go in weeds=>    22:17:18.556561 episode:  400 / 10000  Evaluation Average Reward  -0.250999999999988 Epsilon  0.8644
go in weeds=>    22:17:50.887580 episode:  500 / 10000  Evaluation Average Reward  -0.9999999999999831 Epsilon  0.8554999999999999
go in weeds=>    22:18:22.305511 episode:  600 / 10000  Evaluation Average Reward  2.6339999999999923 Epsilon  0.8465999999999999
go in weeds=>    22:18:54.699514 episode:  700 / 10000  Evaluation Average Reward  1.5000000000000209 Epsilon  0.8376999999999999
go in weeds=>    22:19:29.123635 episode:  800 / 10000  Evaluation Average Reward  1.0190000000000188 Epsilon  0.8287999999999999
go in weeds=>    22:20:04.238512 episode:  900 / 10000  Evaluation Average Reward  1.199040866595169e-14 Epsilon  0.8198999999999999
go in weeds=>    22:20:36.667577 episode:  1000 / 10000  Evaluation Average Reward  -0.910999999999994 Epsilon  0.8109999999999998
